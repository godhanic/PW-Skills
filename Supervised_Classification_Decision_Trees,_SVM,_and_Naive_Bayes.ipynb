{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm1SCK7xEHO6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Classification: Decision Trees, SVM, and Naive Bayes\n"
      ],
      "metadata": {
        "id": "XvdmrOkXEUvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Information Gain (IG) is a key concept used in Decision Trees to decide which feature should be chosen to split the data at each node. It measures how much \"information\" a feature provides about the target variable.\n",
        "\n",
        " Definition:\n",
        "\n",
        "**Information Gain** is based on the concept of **Entropy** (a measure of impurity or disorder in data).\n",
        "\n",
        "Information Gain=Entropy (Parent)− Entropy ( combine Child​)\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "Suppose we’re classifying whether to Play Tennis based on Weather.\n",
        "\n",
        "| Weather  | Play |\n",
        "| -------- | ---- |\n",
        "| Sunny    | No   |\n",
        "| Overcast | Yes  |\n",
        "| Rainy    | Yes  |\n",
        "\n",
        "* Entropy before split = 0.918\n",
        "* After splitting on “Weather”, weighted entropy = 0.5\n",
        "* Information Gain = 0.918 − 0.5 = 0.418\n",
        "\n",
        "This means \"Weather\" reduces uncertainty by 0.418 bits of information.\n"
      ],
      "metadata": {
        "id": "U-mdU_SkL-cY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2 :  What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Here’s a difference between Gini Impurity and Entropy\n",
        "\n",
        "\n",
        "| Feature         | **Gini Impurity**                                                            | **Entropy**                                                   |\n",
        "| --------------- | ---------------------------------------------------------------------------- | ------------------------------------------------------------- |\n",
        "| **Meaning**     | Measures how often a randomly chosen element would be incorrectly classified | Measures the amount of uncertainty or disorder in the dataset |\n",
        "| **Formula**     | ( G=1−∑pi2​ )                                                       | ( (E=−∑pi ​log2​(pi​)) )                                 |\n",
        "| **Range**       | 0 → 0.5 (for 2 classes)                                                      | 0 → 1 (for 2 classes)                                         |\n",
        "| **When it’s 0** | Perfectly pure node (only one class)                                         | Perfectly pure node (only one class)                          |\n",
        "| **Computation** | Simpler, faster (no log calculation)                                         | Slightly slower (uses logarithm)                              |\n",
        "| **Behavior**    | More sensitive to class probabilities                                        | More theoretical and information-based                        |\n",
        "                                \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* Gini → faster and often gives similar results.\n",
        "* Entropy → gives a more “information theory” view of purity.\n",
        "* Both are used to find the best feature to split a decision tree.\n",
        "\n"
      ],
      "metadata": {
        "id": "KKskrPhrPt-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3 : What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "\n",
        "Pre-pruning means stopping the tree from growing too deep while it’s being built — before it starts overfitting the data.\n",
        "\n",
        "During tree building, the algorithm checks certain conditions.\n",
        "If any of these are met, it stops splitting that branch.\n",
        "\n",
        "Common Pre-pruning rules:\n",
        "\n",
        "* Maximum tree depth reached\n",
        "* Minimum number of samples in a node\n",
        "* Information gain (or Gini reduction) is too small\n",
        "* Minimum number of samples required to split not met\n",
        "\n",
        "\n",
        "Pre-pruning, Stop growing the tree early to keep it simple and avoid overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "W5rXAtJ6QgN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 4: Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "#Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load sample dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Create and train the Decision Tree Classifier using Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Print feature names and their importance scores\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(data.feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0140d506-a228-4f9a-c7f6-ed9b9f629638",
        "id": "M4Flv8JXWNzJ"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.013333333333333329\n",
            "sepal width (cm): 0.0\n",
            "petal length (cm): 0.5640559581320451\n",
            "petal width (cm): 0.4226107085346215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks — but it’s mostly known for classification.\n",
        "\n",
        "SVM tries to find the best line (in 2D) or best plane/hyperplane (in higher dimensions)that separates different classes of data as clearly as possible.\n",
        "\n",
        "\n",
        "SVM looks for the maximum margin — the widest gap between data points of different classes.\n",
        "The data points closest to the line/plane are called Support Vectors — they “support” or define the boundary.\n",
        "\n",
        "In short: SVM finds the best boundary that separates different classes with the widest margin possible.\n"
      ],
      "metadata": {
        "id": "9gvvzaFTQfS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "\n",
        "The Kernel Trick is a method used in SVM to handle non-linear data — that is, when the data cannot be separated by a straight line.\n",
        "\n",
        "* Some datasets can’t be divided by a straight line (linear boundary).\n",
        "* The kernel trick transforms the data into a higher-dimensional space,\n",
        "  where it becomes linearly separable.\n",
        "* SVM can then find a linear boundary in that new space — which corresponds to a non-linear boundary in the original space.\n",
        "\n",
        "Example:\n",
        "\n",
        "Imagine data points shaped like two circles — one inside the other.\n",
        "You can’t separate them with a straight line in 2D.\n",
        "But if you map them to 3D (using a kernel), they become separable by a plane.\n",
        "\n",
        "Common Kernels Used:Linear, Polynomial, RBF (Radial Basis Function) and Sigmoid\n",
        "\n",
        "\n",
        "In short:The Kernel Trick allows SVM to separate complex, non-linear data by implicitly mapping it to a higher dimension — without actually computing those dimensions directly."
      ],
      "metadata": {
        "id": "2vNNpQv4SUcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "# Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting on the same dataset.\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create two SVM classifiers with different kernels\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Train both models\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracies\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"Accuracy with Linear Kernel: {acc_linear}\")\n",
        "print(f\"Accuracy with RBF Kernel: {acc_rbf}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9947e97d-21c8-49d2-956b-83fe86b8a37c",
        "id": "3K888GCibtUF"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Linear Kernel: 1.0\n",
            "Accuracy with RBF Kernel: 0.8055555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "\n",
        "Naïve Bayes is a simple and fast probabilistic classifier based on Bayes’ Theorem.\n",
        "It is widely used for text classification, spam detection, and sentiment analysis.\n",
        "\n",
        "\n",
        "It predicts the class of a sample using probabilities —\n",
        "i.e., it calculates the chance that a data point belongs to a particular class.\n",
        "\n",
        "It’s based on Bayes’ Theorem\n",
        "\n",
        "\n",
        "P(A|B) = P(B|A) X P(A) / P(B)\n",
        "\n",
        "Where:\n",
        "\n",
        "* P(A|B)  → Probability of class A given the data B\n",
        "* P(B|A)  → Probability of data B given class A\n",
        "* P(A)  → Prior probability of class A\n",
        "* P(B) → Probability of the data\n",
        "\n",
        "\n",
        "It’s called naïve because it assumes that all features are independent of each other —\n",
        "that is, each feature contributes to the outcome individually, without being related to other features.\n",
        " In reality, this assumption is often not true, but surprisingly, the classifier still works very well in many cases.\n",
        "\n",
        "\n",
        "\n",
        "Example (Spam Detection):\n",
        "\n",
        "If an email contains the words “win,” “money,” and “prize,”\n",
        "Naïve Bayes assumes that these words are independent and calculates probabilities for spam vs. non-spam accordingly.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iC-mbftvUWP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "\n",
        "Here’s a simple and clear comparison of the three main types of Naïve Bayes classifiers\n",
        "\n",
        "\n",
        "| Type                        | Used For                  | Type of Features                                     | Key Idea                                                | Example Use Case                                                                        |\n",
        "| --------------------------- | ------------------------- | ---------------------------------------------------- | ------------------------------------------------------- | --------------------------------------------------------------------------------------- |\n",
        "| **Gaussian Naïve Bayes**    | Continuous (numeric) data | Features follow a **normal (Gaussian)** distribution | Calculates probability using the **bell curve** formula | Predicting diseases based on continuous test values (e.g., blood pressure, temperature) |\n",
        "| **Multinomial Naïve Bayes** | Discrete (count) data     | Features are **counts or frequencies**               | Based on the **multinomial distribution**               | Text classification (e.g., spam detection, word count in emails)                        |\n",
        "| **Bernoulli Naïve Bayes**   | Binary (yes/no) data      | Features are **0 or 1 (True/False)**                 | Works on **presence or absence** of features            | Sentiment analysis or document classification using word presence (word appears or not) |\n",
        "\n",
        "\n",
        "In simple words:\n",
        "\n",
        "* Gaussian NB→ when features are numbers (like height, age, weight).\n",
        "* Multinomial NB → when features are word counts or frequencies.\n",
        "* Bernoulli NB → when features are binary (word present = 1, absent = 0).\n"
      ],
      "metadata": {
        "id": "PGPfBzp9V0UV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 10: Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "# Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from sklearn.datasets.\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Gaussian Naïve Bayes model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Gaussian Naïve Bayes Classifier: {accuracy}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0DSlcM1YXLD",
        "outputId": "c2897f04-9000-43bb-e694-36cca503e5e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes Classifier: 0.9736842105263158\n"
          ]
        }
      ]
    }
  ]
}